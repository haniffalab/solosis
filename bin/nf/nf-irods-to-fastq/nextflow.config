// Global default params, used in configs
params {
    help = false
    // A metadata CSV with the sample IDs, and possibly other iRODS parameters
    // Header relevant, as specifies iRODS metadata field names
    meta = null
    // Other potential arguments, though not mandatory
    // If ATAC, set this to ATAC
    type = null
    // --index-format formula for samtools, only if you really know what you're doing
    index_format = "i*i*"
    // Whether to publish fastqs - other workflows using this may not want to
    publish_fastqs = true
    // Where to put the output (publish folder path)
    publish_dir = "results"
    // Advanced CRAM list manipulation
    // Only return the found SAMPLE,CRAM list, e.g. for manual curation/manipulation
    find_crams_only = false
    // Accept SAMPLE,CRAM list on input
    cram_list = null
    // Merge multiple lanes of the same sample into a single sample FASTQ (useful for ArrayExpress/Biostudies)
    merge = false
    // FTP upload config (useful for ArrayExpress/Biostudies)
    ftp_upload = false
    ftp_credentials = 'aexpress,aexpress1'
    ftp_host = 'ftp-private-2.ebi.ac.uk'
    ftp_path = null
    // By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by 
    // $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI)
    // reference genome server, and further falling back to the @SQ “UR” field if these are not found.
    REF_PATH="/lustre/scratch125/core/sciops_repository/cram_cache/%2s/%2s/%s:/lustre/scratch126/core/sciops_repository/cram_cache/%2s/%2s/%s:URL=http:://sf2-farm-srv1.internal.sanger.ac.uk::8000/%s" 
}

// Singularity environment parameters
singularity {
  enabled     = true
  autoMounts  = true
  runOptions  = '-B /lustre,/nfs'
}

// Configuring LSF job submission
executor {
    name = 'lsf'
    perJobMemLimit = true
}

process {
    withLabel: normal {
        queue  = "normal"
        cpus   = 1
        memory = 2.GB
    }
    withLabel: normal4core {
        queue  = "normal"
        cpus   = 4
        memory = 2.GB
    }
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

// Capturing Nextflow log files into a 'reports' directory
import java.time.*
Date now = new Date()

params {
    tracedir = "reports"
    timestamp = now.format("yyyyMMdd-HH-mm-ss")
}

timeline {
    enabled = false
    file = "${params.tracedir}/${params.timestamp}_timeline.html"
}
report {
    enabled = false
    file = "${params.tracedir}/${params.timestamp}_report.html"
}

// Ensure work directories and removed on successfull pipeline execution
//cleanup = true

